{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## SKlearn Overview\n",
    "Outline:\n",
    "* Datasets\n",
    "* Splitting data into test/train/validation sets\n",
    "* Learning and predicting\n",
    "* Parameter tuning\n",
    "* Model persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Loading builtin dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Provides toy datasets\n",
    "from sklearn import datasets\n",
    "# Load the iris dataset for classification\n",
    "iris = datasets.load_iris()\n",
    "# load the digits dataset for classification\n",
    "digits = datasets.load_digits()\n",
    "# load boston housing price for regression\n",
    "boston = datasets.load_boston()\n",
    "# load diabetes dataset for regression\n",
    "diabetes = datasets.load_diabetes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding dataset\n",
    "Check dataset object.<tab> to see various members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris feature names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "data type: <class 'numpy.ndarray'>\n",
      "[[ 5.1  3.5  1.4  0.2]\n",
      " [ 4.9  3.   1.4  0.2]\n",
      " [ 4.7  3.2  1.3  0.2]\n",
      " [ 4.6  3.1  1.5  0.2]\n",
      " [ 5.   3.6  1.4  0.2]\n",
      " [ 5.4  3.9  1.7  0.4]\n",
      " [ 4.6  3.4  1.4  0.3]\n",
      " [ 5.   3.4  1.5  0.2]\n",
      " [ 4.4  2.9  1.4  0.2]\n",
      " [ 4.9  3.1  1.5  0.1]]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"iris feature names: {}\".format(iris.feature_names))\n",
    "print(\"data type: {}\".format(type(iris.data)))\n",
    "print(iris.data[:10])\n",
    "print(iris.target[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### train_test_split\n",
    "Splitting data into validation, testing and training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "# 20% of data as testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\n",
    "# split 100 of TRAINING data as validation set\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=100, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make classification dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 20)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.23726495, -1.42694126, -1.13635742, -0.11537417,  0.473066  ,\n",
       "        -1.48315945, -0.7857967 ,  0.22322429, -0.36297705,  0.32656592,\n",
       "         0.50851831, -0.52467883,  0.79906067,  0.16989665, -0.12470124,\n",
       "         0.94293164, -0.69146619,  0.22574145, -0.38134688, -0.15187249],\n",
       "       [-1.17733603,  2.26521396,  2.16427811,  0.6082616 , -1.15744382,\n",
       "        -0.7847868 ,  0.74406337, -0.22209533, -0.02985833,  0.50763087,\n",
       "         0.2617405 ,  2.4095089 , -0.2831095 , -0.12451756,  0.17816058,\n",
       "        -1.30873356,  1.35069157,  0.42397519,  0.49397065,  0.13951352],\n",
       "       [ 1.86981387,  1.45532944,  0.83273762, -1.81056441,  1.22342838,\n",
       "        -1.04625454,  1.360467  ,  0.56656878, -2.41948547, -0.02484754,\n",
       "        -0.67810678, -0.86777774,  1.40228871,  0.09922184,  0.25624359,\n",
       "        -1.13200721,  0.11634595,  1.2644228 ,  0.1573534 ,  0.24685201],\n",
       "       [ 1.32072937,  2.52199323,  0.47945114, -0.23801398,  0.83387694,\n",
       "        -0.43956229, -0.31523718,  1.09958603, -0.38909473,  1.10818432,\n",
       "        -0.100513  ,  0.55449431, -0.63749102, -0.06656541, -0.89868724,\n",
       "        -2.46478815, -0.34742556,  1.90601558, -2.26142023,  0.69941057],\n",
       "       [-2.87797795, -2.73824161, -0.16933737,  0.9280283 ,  0.33847992,\n",
       "         1.70413899,  0.19481662,  0.05514078,  0.03450395, -0.53722203,\n",
       "        -0.49952966, -1.8890791 ,  0.33852994,  0.64883014,  1.32591528,\n",
       "         2.85949856, -0.29526329, -0.39945155, -1.67932995, -0.858386  ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=100, n_features=20)\n",
    "print(X.shape)\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train a SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the classifier\n",
    "from sklearn import svm\n",
    "# C is a hyper-parameter\n",
    "clf = svm.SVC(C=10)\n",
    "# Training a classifier\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy = 0.72\n"
     ]
    }
   ],
   "source": [
    "# Predict on the validation set to see accuracy\n",
    "import numpy as np\n",
    "predictions = clf.predict(X_valid)\n",
    "print('validation accuracy = {}'.format(np.sum(predictions == y_valid)/len(y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy = 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict(X_test)\n",
    "print ('test accuracy = {}'.format(np.sum(predictions == y_test)/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Model Parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at C = 1.0 is 0.79\n",
      "Accuracy at C = 2.0 is 0.73\n",
      "Accuracy at C = 3.0 is 0.72\n",
      "Accuracy at C = 4.0 is 0.7\n",
      "Accuracy at C = 5.0 is 0.69\n",
      "Accuracy at C = 6.0 is 0.72\n",
      "Accuracy at C = 7.0 is 0.72\n",
      "Accuracy at C = 8.0 is 0.72\n",
      "Accuracy at C = 9.0 is 0.73\n",
      "Accuracy at C = 10.0 is 0.72\n",
      "Accuracy at C = 11.0 is 0.7\n",
      "Accuracy at C = 12.0 is 0.7\n",
      "Accuracy at C = 13.0 is 0.69\n",
      "Accuracy at C = 14.0 is 0.69\n",
      "Accuracy at C = 15.0 is 0.65\n",
      "Accuracy at C = 16.0 is 0.64\n",
      "Accuracy at C = 17.0 is 0.64\n",
      "Accuracy at C = 18.0 is 0.64\n",
      "Accuracy at C = 19.0 is 0.64\n",
      "Best C = 1.0. It has an accuracy of 0.79\n",
      "final test accuracy = 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "# How to select the best value of C?\n",
    "# See the value of C that gives best accuracy on validation data\n",
    "best_acc = 0.0\n",
    "best_C = 0.0\n",
    "step_size = 1.0\n",
    "C = 1.0\n",
    "while C < 20.0:\n",
    "    clf = svm.SVC(C=C)\n",
    "    clf.fit(X_train, y_train)\n",
    "    accuracy = np.sum(clf.predict(X_valid)==y_valid)/len(y_valid)\n",
    "    print ('Accuracy at C = ' + str(C) + ' is ' + str(accuracy))\n",
    "    if (accuracy > best_acc):\n",
    "        best_acc = accuracy\n",
    "        best_C = C\n",
    "    C += step_size\n",
    "print ('Best C = ' + str(best_C) + '. It has an accuracy of ' + str(best_acc))\n",
    "\n",
    "clf = svm.SVC(C=best_C)\n",
    "# after tuning parameter, we want use whole data available to train the model for better accuracy\n",
    "X_train_valid = np.concatenate((X_train,X_valid))\n",
    "y_train_valid = np.concatenate((y_train,y_valid))\n",
    "clf.fit(X_train_valid, y_train_valid)\n",
    "predictions = clf.predict(X_test)\n",
    "print ('final test accuracy = {}'.format(np.sum(predictions == y_test)/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch: auto search best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 19 candidates, totalling 57 fits\n",
      "Best C:{'C': 1}, final test accuracy = 0.8333333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  57 out of  57 | elapsed:    1.5s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "parameters = { 'C':[i for i in range(1,20)] }\n",
    "estimator = svm.SVC()\n",
    "clf = GridSearchCV(estimator, parameters, verbose=True, n_jobs=-1)\n",
    "clf.fit(X_train_valid, y_train_valid)\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_pred, y_test)\n",
    "print ('Best C:{}, final test accuracy = {}'.format(clf.best_params_, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Model persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It is possible to save a model in the scikit by using Python’s built-in persistence model, namely pickle \n",
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "clf = svm.SVC()\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "clf.fit(X, y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "s = pickle.dumps(clf)\n",
    "clf2 = pickle.loads(s)\n",
    "pred = clf2.predict(X[0:1])\n",
    "print (pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In the specific case of the scikit, it may be more interesting to use joblib’s replacement of pickle (joblib.dump & joblib.load), which is more efficient on big data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['filename.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(clf, 'filename.pkl')  #.pkl means a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clf = joblib.load('filename.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Other type of models such as regressors, clustering mechansims etc. will be discussed later. This module was only to give a brief overview of the capabilities of sklearn"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
