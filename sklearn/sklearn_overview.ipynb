{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## SKlearn Overview\n",
    "Outline:\n",
    "* Datasets\n",
    "* Splitting data into test/train/validation sets\n",
    "* Learning and predicting\n",
    "* Parameter tuning\n",
    "* Model persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Loading builtin dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Provides toy datasets\n",
    "from sklearn import datasets\n",
    "# Load the iris dataset for classification\n",
    "iris = datasets.load_iris()\n",
    "# load the digits dataset for classification\n",
    "digits = datasets.load_digits()\n",
    "# load boston housing price for regression\n",
    "boston = datasets.load_boston()\n",
    "# load diabetes dataset for regression\n",
    "diabetes = datasets.load_diabetes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding dataset\n",
    "Check dataset object.<tab> to see various members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris feature names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "data type: <class 'numpy.ndarray'>\n",
      "[[ 5.1  3.5  1.4  0.2]\n",
      " [ 4.9  3.   1.4  0.2]\n",
      " [ 4.7  3.2  1.3  0.2]\n",
      " [ 4.6  3.1  1.5  0.2]\n",
      " [ 5.   3.6  1.4  0.2]\n",
      " [ 5.4  3.9  1.7  0.4]\n",
      " [ 4.6  3.4  1.4  0.3]\n",
      " [ 5.   3.4  1.5  0.2]\n",
      " [ 4.4  2.9  1.4  0.2]\n",
      " [ 4.9  3.1  1.5  0.1]]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"iris feature names: {}\".format(iris.feature_names))\n",
    "print(\"data type: {}\".format(type(iris.data)))\n",
    "print(iris.data[:10])\n",
    "print(iris.target[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### train_test_split\n",
    "Splitting data into validation, testing and training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "# 20% of data as testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\n",
    "# split 100 of TRAINING data as validation set\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=100, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make classification dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 20)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.03187562,  0.82716458,  0.26624028,  0.70673893,  0.29778841,\n",
       "        -0.30390553, -0.26306736,  1.83674665, -1.76217736,  0.43227414,\n",
       "        -0.14297423,  0.65253384, -1.29531075,  0.35103968, -0.46424475,\n",
       "        -1.44701875,  0.56442579, -0.03585862, -0.67444898,  0.28761468],\n",
       "       [ 1.17102087, -0.71576384, -1.46558959,  0.60009953,  0.83085292,\n",
       "         0.63801459, -1.72519809,  0.97858921, -0.99166435, -0.92631696,\n",
       "        -1.75468392, -0.67310765,  0.79692589,  0.66883924,  0.39360306,\n",
       "         0.9215216 , -0.04924816, -0.39999352,  0.61130867,  0.33061983],\n",
       "       [ 1.55387779,  2.34902858,  1.29002563, -0.60084608, -0.77714443,\n",
       "         0.19967495,  0.9546367 ,  1.6147857 , -1.61398441,  0.31679408,\n",
       "         0.48104134,  1.18135166, -0.08628892, -1.22970298,  1.59280076,\n",
       "         0.80340432,  0.45300414, -0.49862761,  0.75625604,  1.384176  ],\n",
       "       [-1.19767151, -0.23115988, -0.26439326,  0.2047244 , -0.11121745,\n",
       "         0.40269741,  0.3085281 , -3.43548964,  3.3872581 , -1.80177188,\n",
       "        -0.77229341,  0.50341898, -0.94699226, -0.23563015,  2.05403003,\n",
       "        -0.21799956,  2.07086718,  0.72523404,  0.48573669,  0.54729276],\n",
       "       [-0.02566634,  0.46809932,  0.49914782,  0.51724866, -0.83484315,\n",
       "        -0.26703849, -0.8792267 , -3.23334417,  3.1901469 ,  1.30423053,\n",
       "         1.11112972, -0.1094384 , -0.49505685, -1.35793841,  0.84276518,\n",
       "        -0.2755678 ,  1.65732267,  0.69840342,  0.45448009,  0.13884613]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=100, n_features=20)\n",
    "print(X.shape)\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train a SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the classifier\n",
    "from sklearn import svm\n",
    "# C is a hyper-parameter\n",
    "clf = svm.SVC(C=10)\n",
    "# Training a classifier\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy = 0.72\n"
     ]
    }
   ],
   "source": [
    "# Predict on the validation set to see accuracy\n",
    "import numpy as np\n",
    "predictions = clf.predict(X_valid)\n",
    "print('validation accuracy = {}'.format(np.sum(predictions == y_valid)/len(y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy = 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict(X_test)\n",
    "print ('test accuracy = {}'.format(np.sum(predictions == y_test)/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Model Parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at C = 0.1 is 0.51\n",
      "Accuracy at C = 5.1 is 0.69\n",
      "Accuracy at C = 10.1 is 0.72\n",
      "Accuracy at C = 15.1 is 0.65\n",
      "Accuracy at C = 20.1 is 0.64\n",
      "Accuracy at C = 25.1 is 0.64\n",
      "Accuracy at C = 30.1 is 0.65\n",
      "Accuracy at C = 35.1 is 0.65\n",
      "Accuracy at C = 40.1 is 0.65\n",
      "Accuracy at C = 45.1 is 0.64\n",
      "Accuracy at C = 50.1 is 0.64\n",
      "Accuracy at C = 55.1 is 0.64\n",
      "Accuracy at C = 60.1 is 0.65\n",
      "Accuracy at C = 65.1 is 0.65\n",
      "Accuracy at C = 70.1 is 0.66\n",
      "Accuracy at C = 75.1 is 0.66\n",
      "Accuracy at C = 80.1 is 0.67\n",
      "Accuracy at C = 85.1 is 0.67\n",
      "Accuracy at C = 90.1 is 0.67\n",
      "Accuracy at C = 95.1 is 0.67\n",
      "Best C = 10.1. It has an accuracy of 0.72\n",
      "final test accuracy = 0.9\n"
     ]
    }
   ],
   "source": [
    "# How to select the best value of C?\n",
    "# See the value of C that gives best accuracy on validation data\n",
    "best_acc = 0.0\n",
    "best_C = 0.0\n",
    "step_size = 5.0\n",
    "C = 0.1\n",
    "while C < 100.0:\n",
    "    clf = svm.SVC(C=C)\n",
    "    clf.fit(X_train, y_train)\n",
    "    accuracy = np.sum(clf.predict(X_valid)==y_valid)/len(y_valid)\n",
    "    print ('Accuracy at C = ' + str(C) + ' is ' + str(accuracy))\n",
    "    if (accuracy > best_acc):\n",
    "        best_acc = accuracy\n",
    "        best_C = C\n",
    "    C += step_size\n",
    "print ('Best C = ' + str(best_C) + '. It has an accuracy of ' + str(best_acc))\n",
    "\n",
    "clf = svm.SVC(C=best_C)\n",
    "# after tuning parameter, we want use whole data available to train the model for better accuracy\n",
    "X_train_valid = np.concatenate((X_train,X_valid))\n",
    "y_train_valid = np.concatenate((y_train,y_valid))\n",
    "clf.fit(X_train_valid, y_train_valid)\n",
    "predictions = clf.predict(X_test)\n",
    "print ('final test accuracy = {}'.format(np.sum(predictions == y_test)/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Model persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It is possible to save a model in the scikit by using Python’s built-in persistence model, namely pickle \n",
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "clf = svm.SVC()\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "clf.fit(X, y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "s = pickle.dumps(clf)\n",
    "clf2 = pickle.loads(s)\n",
    "pred = clf2.predict(X[0:1])\n",
    "print (pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In the specific case of the scikit, it may be more interesting to use joblib’s replacement of pickle (joblib.dump & joblib.load), which is more efficient on big data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(clf, 'filename.pkl')  #.pkl means a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clf = joblib.load('filename.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Other type of models such as regressors, clustering mechansims etc. will be discussed later. This module was only to give a brief overview of the capabilities of sklearn"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
